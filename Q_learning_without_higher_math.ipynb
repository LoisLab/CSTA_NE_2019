{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q-learning without higher math.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii-7Q4iy1Xlq",
        "colab_type": "text"
      },
      "source": [
        "# Q-learning without higher math\n",
        "\n",
        "### Can one algorithm learn to solve many problems?\n",
        "\n",
        "If I asked you to write a program to solve a maze, you might think about how a maze works, and ask *which way to the exit?* If I asked you write a program to play tic-tac-toe, you might start by thinking about the board, or the rules, or whether to go first.\n",
        "\n",
        "But: what if I asked you to write a program that could learn to solve either a maze or tic-tac-toe, without changing a single line of code?\n",
        "\n",
        "Does that sound possible?\n",
        "\n",
        "### Q-learning\n",
        "\n",
        "Let's take a journey from conventional programming toward machine learning. You will use a simple maze as a sample problem. By applying a algorithm called **Q-learning**, you won't just find the way through the maze. You will demonstrate a way to solve a whole class of problems, by creating programs that learn as they go.\n",
        "\n",
        "You will be working in the field of **reinforcement learning**, a major branch of machine learning. RL confronts problem that evolve one step at a time, like playing a game, driving a car, or building spooky dog-shaped robots. Other major branches deal with problems like image or speech recognition. You will be playing games, not looking for cats. Looking for cats is a totally different problem.\n",
        "\n",
        "### Calculus: not required\n",
        "\n",
        "You won't need calculus, probability & statistics, or linear algebra to complete your journey. If you know those topics, that's great... those would allow you to solve problems with larger dimensions, like playing Super Mario Bros or chess. **However: the principles that guide Q-learning are exactly the same, with or without the higher math.** The purpose of the math is to make approximations when a problem's dimensions get out of hand. The core algorithm is unchanged.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbMRtvGqn_kk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title (run this cell to load machine learning environment) { vertical-output: true, display-mode: \"form\" }\n",
        "import numpy as np\n",
        "from mpl_toolkits.mplot3d import axes3d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class Maze():\n",
        "    # nicknames for actions... for beginners, only\n",
        "    actions = ['N','S','E','W']\n",
        "    # offsets to move North, South, East, or West\n",
        "    offset = [(-1,0),(1,0),(0,1),(0,-1)]\n",
        "\n",
        "    def __init__(self):\n",
        "        self.maze = np.array([[0,0,0,-1],[0,-1,0,0],[0,0,-1,0],[-1,-1,0,0]])  # the maze is hardcoded\n",
        "        self.mark = None\n",
        "        self.reset()\n",
        "\n",
        "    # clear the blocks, leaving an empty maze\n",
        "    def remove_blocks(self):\n",
        "        self.maze = np.zeros((4,4),dtype=int)\n",
        "    \n",
        "    # reset the environment\n",
        "    def reset(self, random=False):\n",
        "        self.i = 1\n",
        "        self.maze[0][0] = self.i  # mark initial position with counter\n",
        "        self.player = (0,0)\n",
        "        return self.player\n",
        "\n",
        "    # take one step, using a specified action\n",
        "    def step(self, action):\n",
        "        self.i += 1\n",
        "        if type(action) is str:\n",
        "            action = Maze.actions.index(action)\n",
        "        obs = tuple(np.add(self.player, Maze.offset[action]))\n",
        "        if max(obs) > 3 or min(obs) < 0 or self.maze[obs] == -1:\n",
        "            # player is out of bounds or square is blocked... don't move player\n",
        "            return self.player, -1, True\n",
        "        else:\n",
        "            # move was successful, advance player to new position\n",
        "            self.maze[obs] = self.i\n",
        "            self.player = obs\n",
        "            if np.array_equal(self.player, (3,3)):  # reached the exit\n",
        "                return self.player, 1, True\n",
        "            else:\n",
        "                return self.player, 0, False        # no outcome (player is on an open space)\n",
        "\n",
        "    # return a random action (equally distributed across the action space)\n",
        "    def sample(self):\n",
        "        return Maze.actions[np.random.randint(4)]\n",
        "\n",
        "    # return a random action in numeric form (equally distributed across the action space)\n",
        "    def sample_n(self):\n",
        "        return np.random.randint(4)\n",
        "\n",
        "    def action_space(self):\n",
        "        return Maze.actions\n",
        "\n",
        "    def action_space_n(self):\n",
        "        return [0,1,2,3]\n",
        "\n",
        "    def state_space(self):\n",
        "        return 4,4\n",
        "\n",
        "    def __str__(self):\n",
        "        out = '\\n=========='\n",
        "        for x in range(4):\n",
        "            out += '\\n|'\n",
        "            for y in range(4):\n",
        "                if self.mark is not None and self.mark[0]==x and self.mark[1]==y:\n",
        "                    out += '? '\n",
        "                elif self.maze[x][y]>0:\n",
        "                    out += str(self.maze[x][y]) + ' '\n",
        "                elif self.maze[x][y]==-1:\n",
        "                    out += 'X '\n",
        "                elif x==3 and y==3:\n",
        "                    out += '* '\n",
        "                else:\n",
        "                    out += '. '\n",
        "            out += '|'\n",
        "        out += '\\n==========\\n'\n",
        "        return out\n",
        "\n",
        "    def print_q(q, mode='all'):\n",
        "        print('=====  ================================')\n",
        "        print('state         N       S       E       W\\n')\n",
        "        for x in range(4):\n",
        "            for y in range(4):\n",
        "                out = '('\n",
        "                out += str(x)\n",
        "                out += ','\n",
        "                out += str(y)\n",
        "                out += ')  '\n",
        "                for a in range(4):\n",
        "                    if mode=='all':\n",
        "                        out += '{:>8,d}'.format(int(q[x][y][a]))\n",
        "                    elif mode=='rewards':\n",
        "                        if q[m][n][a] > 0:\n",
        "                            out += '{:8.3f}'.format(q[x][y][a])\n",
        "                        else:\n",
        "                            out += '        '\n",
        "                print(out)\n",
        "            print('-----  --------------------------------')\n",
        "\n",
        "    def plot(q):\n",
        "      fig = plt.figure(figsize=(16,6))\n",
        "      ax1 = fig.add_subplot(121, projection='3d')\n",
        "      x,y,b = [],[],[]\n",
        "      z = np.zeros((4,4))\n",
        "      for m in range(4):\n",
        "        for n in range(4):\n",
        "          x.append(m)\n",
        "          y.append(n)\n",
        "          b.append(0)\n",
        "          z[m][n] += max(max(q[m][n]),0)\n",
        "\n",
        "      ax1.bar3d(x, y, b, 1, 1, np.ravel(z), shade=True)\n",
        "      plt.title('Positive Q-scores')\n",
        "      plt.xlabel('state (row)')\n",
        "      plt.ylabel('state (col)')\n",
        "      plt.show()\n",
        "\n",
        "print('the maze environment is all set to go')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj714p4F879p",
        "colab_type": "text"
      },
      "source": [
        "# Let's have a look at the maze\n",
        "\n",
        "### Machine learning environments\n",
        "\n",
        "Many machine learning problems are packaged into **environments**. An environment is a program that presents a problem in a rigorous, predictable way. When you want to write an algorithm to solve more than one problem, environments are a big help, because you can take your algorithm and aim it at more than one problem.\n",
        "\n",
        "Facilities like the **OpenAI Gym** are filled with machine learning environments. You can connect to any of them in more or less the same way. Take a look, [here](https://gym.openai.com/envs/#classic_control).\n",
        "\n",
        "Your first environment represents a 4x4 maze. The maze is engineered just like the environments at OpenAI. Once you solve the maze, you can head over to the gym and take a crack at something more complex.\n",
        "\n",
        "You can create and view the maze like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egJRU15btiBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = Maze()     # make an instance of the maze environment\n",
        "print(m)       # print the maze"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI_hYEGt-nyg",
        "colab_type": "text"
      },
      "source": [
        "# Stepping through the maze\n",
        "\n",
        "Like many environments, the maze provides functions to reset to an initial state, and to take one step:\n",
        "\n",
        "```\n",
        "reset()        # start over, from the beginning\n",
        "step(action)   # take a step ('N','S','E','W')\n",
        "```\n",
        "\n",
        "You can take your very first step in the maze like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtGqIkhD_u1a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Maze()     # make an instance of the maze environment\n",
        "env.reset()      # reset to initial state (always a good idea)\n",
        "env.step('E')    # take a step to the east...\n",
        "print(env)       # ...where am I now?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XT8mKlwI_3A3",
        "colab_type": "text"
      },
      "source": [
        "Notice the numbering that goes from (1) to (2)? That's you, taking one step to the east.\n",
        "\n",
        "Since you know a thing or two about mazes, you could solve the problem like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWGH3bGK_Of2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Maze()\n",
        "env.reset()\n",
        "actions = ['E','E','S','E','S','S']  # these actions navigate the maze\n",
        "print(env)\n",
        "\n",
        "for action in actions:               # loop over actions\n",
        "    env.step(action)                 # take a step...\n",
        "    print(env)                       # ...print the maze\n",
        "    \n",
        "print('I found the exit!')           # of course you did / you know about mazes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B5ks9cwR5da",
        "colab_type": "text"
      },
      "source": [
        "# Is being a maze expert really necessary?\n",
        "\n",
        "You did good work solving the maze, but at what cost? Your solution won't get you through another equally simple maze, much less solve an entirely different problem. You did what you were asked -- you solved a specific problem -- but that approach loses **generality**. It won't work beyond this one maze.\n",
        "\n",
        "### It's no longer a maze. It's just a problem.\n",
        "\n",
        "From here on, let's turn the problem inside-out: forget everything you know about mazes. Instead, ask yourself: how few things could I know and still solve the maze, if I am willing to learn by trial and error? And since that approach avoids **specific** knowledge, does it open the possibility of a **general** solution, that could solve all sorts of problems?\n",
        "\n",
        "Starting over: you need to solve a problem. You don't know the nature of the problem. What are the fewest things you could know, to attempt a solution, no matter how error-prone or inefficient?\n",
        "\n",
        "\n",
        "### In general: what class of problems are you attempting to solve?\n",
        "\n",
        "If you had to list characteristics for a general class of problems like games or a maze, what would you include?\n",
        "\n",
        "How about:\n",
        "\n",
        "1. the problem always exists in one (and only one) **state**\n",
        "2. a chosen **action** causes the problem to transit from one to another state\n",
        "3. the **current** state of the problem does not depend on any **prior** state\n",
        "4. the problem signals if you succeed or fail (using a numeric **reward** or **penalty**)\n",
        "5. the problem signals if the attempt is over (by raising a boolean **done** flag)\n",
        "6. you are allowed to start over, repeatedly, because: **mistakes**\n",
        "\n",
        "In general, how many problems could be described that way? Almost every game you've ever played? Landing on the moon? Finding your way through a maze?\n",
        "\n",
        "### Approach the maze as a general problem\n",
        "\n",
        "Try the maze again, but this time, forget it's a maze. Don't use the print function. Instead, listen to the feedback provided by the maze environment:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33UGwJNkVSsV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "machine learning environments provide feedback, including an\n",
        "initial state and the dimensions of the state and action spaces.\n",
        "'''\n",
        "\n",
        "env = Maze()\n",
        "obs = env.reset()  # the environment returns an observation of the initial state\n",
        "print(obs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5mxlXtJV73_",
        "colab_type": "text"
      },
      "source": [
        "OK, the **state** appears to be two-dimensional. What will the environment tell you about the dimensions of the **state space**, which is a description of the dimensions of all possible states?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZyAbStHWGhf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "A 'space' is some collection of discrete or continous possibilities... an egg\n",
        "carton can be described as a 6x2 space, with an overall dimesion of 12. A chess\n",
        "board is 8x8, but any of the spaces could contain any piece, so the actual state\n",
        "space is much larger than 8x8=64. In fact, it is fantastically gigantic.\n",
        "'''\n",
        "\n",
        "# what is the state space for the problem?\n",
        "print(env.state_space())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VWBUunCWN6W",
        "colab_type": "text"
      },
      "source": [
        "Looks like the state space is 4x4. That means: no matter what happens, you will be in a bounded space, from (0,0) to (3,3).\n",
        "\n",
        "What about the available **actions**? How many possible actions are there?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxrwd7VyWVMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Action spaces can be tricky. For example, in tic-tac-toe, there are 9 actions,\n",
        "representing an attempt to claim any of the 9 spaces of the board. Here's the\n",
        "tricky part: there are always 9 actions, regardless of the state of the game.\n",
        "Putting an 'X' on top of and 'O' is an action -- it's also a bad idea -- but\n",
        "you would learn that from experience, not from a diminished state space.\n",
        "'''\n",
        "\n",
        "# what is the action space for the problem?\n",
        "print(env.action_space())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KADuMIHyWY5U",
        "colab_type": "text"
      },
      "source": [
        "Hmmm. The four available actions have cute nicknames: N,S,E,W. The nicknames aren't necessary -- in fact, those will become a nuisance -- but for now, they will help you remember which action is which.\n",
        "\n",
        "The environment also provides a simpler representation of available actions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYFlrbBJwjqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# what is the numeric representation of the action space?\n",
        "print(env.action_space_n())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldjfiCwjxcsL",
        "colab_type": "text"
      },
      "source": [
        "That makes sense. There are four actions: 0,1,2,3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ul0lSkn9x2Dg",
        "colab_type": "text"
      },
      "source": [
        "# Take a step, with feedback\n",
        "\n",
        "Try taking a step or two, while paying attention to the feedback provided by the function step(action). Like many reinforcement learning environments, the maze returns three values when you take a step:\n",
        "1. an observation of the updated state\n",
        "2. a numeric reward (or, if negative, a numeric penalty)\n",
        "3. a 'done' flag (True means 'game over')\n",
        "\n",
        "Try capturing those return values, and examining each individually:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmQnFRhQy1w_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Maze()\n",
        "\n",
        "# resetting the environment returns the initial state\n",
        "obs = env.reset()\n",
        "print('my initial state is', obs)\n",
        "\n",
        "# taking a step returns an new observation, a reward, and a done flag\n",
        "print('\\nI am about to take an action called E... \\n...here goes nothing...\\n')\n",
        "obs, reward, done = env.step('E')\n",
        "print('my new state is:      ', obs)\n",
        "print('my reward/penalty is: ', reward)\n",
        "print('am I done?            ', done)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z3K0plr0A97",
        "colab_type": "text"
      },
      "source": [
        "What exactly do those values represent? Go ahead and print the maze, then take a look."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06YjZBQK1ot-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3_t6rjD1wju",
        "colab_type": "text"
      },
      "source": [
        "See what happened? Your new state (1,0) represents the position to the east of the entrance. You did not receive a reward -- that's only available at the exit -- nor did you receive a penalty, because nothing went wrong. And is the game over? Not yet.\n",
        "\n",
        "Trying making a mistake. From your new position, go south, and smack into the blocked square:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dwrhO-N2MKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\nI am about to take an action called S... \\n...here goes nothing...\\n')\n",
        "obs, reward, done = env.step('S')\n",
        "print('my new state is:      ', obs)\n",
        "print('my reward/penalty is: ', reward)\n",
        "print('am I done?            ', done)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7uORKKz2UW3",
        "colab_type": "text"
      },
      "source": [
        "Get it? Your new state (1,1) is not a good place to be. You received a penalty of -1, and the game ended. That's not all bad. By making a mistake, you found out what not to do. You can always start again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFTukcEV2ksP",
        "colab_type": "text"
      },
      "source": [
        "# Solve the maze with a random walk\n",
        "\n",
        "What is the minimal algorithm that might solve the maze, now that you recognize the feedback available from step(action)? How about making random moves, until you either succeed, or fail? That's not really machine learning, because you are not taking advantage of experience in each successive attempt, but at least it's a solution.\n",
        "\n",
        "And: a random walk will show you how to **explore** an environment, without knowing anything specific.\n",
        "\n",
        "To make things easier, machine learning environments provide a sample() function, which returns a move drawn from a random distribution:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIZ0o7Uk3sBs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the environment will sample the action space for you\n",
        "env = Maze()\n",
        "print('10 sample actions using nicknames:',[env.sample() for x in range(10)])\n",
        "print('10 sample actions using numbers:  ',[env.sample_n() for x in range(10)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdDFAxD452aE",
        "colab_type": "text"
      },
      "source": [
        "Once you have random actions, it's easy enough to take a random walk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDIJ-m8i578P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Maze()\n",
        "env.reset()\n",
        "total_steps = 0\n",
        "while True:\n",
        "    # take a random step\n",
        "    obs,reward,done = env.step(env.sample())\n",
        "    total_steps += 1\n",
        "    if done:             # is the game over?\n",
        "        if reward>0:     #    did you earn a positive reward?\n",
        "            break        #       sucess! stop walking around\n",
        "        else:            #    else\n",
        "            env.reset()  #       failure... try again\n",
        "            \n",
        "print('I found my way out, and it only took', total_steps, 'steps...')\n",
        "print('...I ignored the observations. Who needs those, anyway?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPKUIUzqDg0e",
        "colab_type": "text"
      },
      "source": [
        "Well, you found the exit. That's an important result, because it demonstrates that the problem has a solution.\n",
        "\n",
        "If you can find the exit at random, could you *learn* to find the exit, from experience?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHPCfQivD9lt",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning: the art of learning from mistakes\n",
        "\n",
        "If you took a first step in the maze, and it went badly, would you take the same step, again?\n",
        "\n",
        "Probably not (*probably in the mathematical sense... lots of environments change at random; the maze happens to be static*).\n",
        "\n",
        "Rather than repeating an mistake, you would probably learn from experience.\n",
        "\n",
        "Here's an example of things going wrong, on the very first step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqbC_UjlJRa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make a maze, then step right out of bounds.\n",
        "env = Maze()\n",
        "env.reset()\n",
        "print('\\nI am about to take an action called N... \\n...here goes nothing...\\n')\n",
        "obs, reward, done = env.step('N')\n",
        "print('my new state is:      ', obs)\n",
        "print('my reward/penalty is: ', reward)\n",
        "print('am I done?            ', done)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrK4DCu4JfN7",
        "colab_type": "text"
      },
      "source": [
        "That feedback says: *you are out of bounds, you received a penalty of -1, and the game is over.*\n",
        "\n",
        "Seems like something worth remembering. How would you keep track of that result, in plain english?\n",
        "\n",
        "You might say: *hey, on the first step, don't choose N.*\n",
        "\n",
        "That might help, but it's not quite as general as it could be. Does it really matter if you are on the first step? What happens if you make the same mistake, on the third step?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph6eca71KYx6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try going S,N,N\n",
        "env = Maze()\n",
        "env.reset()\n",
        "print('Here I am at the starting point')\n",
        "print(env)\n",
        "print('Am taking the action called S')\n",
        "env.step('S')\n",
        "print(env)\n",
        "print('Am taking the action called N')\n",
        "env.step('N')\n",
        "print(env)\n",
        "print('And why not? Trying N, again')\n",
        "print('...oops...',env.step('N'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng5je-rpLnpq",
        "colab_type": "text"
      },
      "source": [
        "Now you can see a more general way to describe your experience: **if you are in state(0,0), don't choose action N.** It does not matter how you arrived at (0,0). All that matters is: when your state is (0,0), avoid action N: <code>(0,0)+N->bad</code>.\n",
        "\n",
        "In more formal terms: a **state-action transition** is the result of taking an action from a given state, causing you to transit to a new state. A notation for an initial move to the south might be:\n",
        "\n",
        "<code>(0,0)+S->(0,1)</code>\n",
        "\n",
        "In other words: if you are in state (0,0) and take action S, you will transit to state (0,1):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9WQ9iRc4iBa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "example of a state-action transition.\n",
        "from initial state, take action S:\n",
        "'''\n",
        "env = Maze()\n",
        "state = env.reset()\n",
        "action = 'S'\n",
        "new_state, _, __ = env.step(action)\n",
        "print('state-action transition is: ', state, '+', action, '->', new_state)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOzfMg9Y5M3u",
        "colab_type": "text"
      },
      "source": [
        "# Remembering rewards or penalties using a Q-table\n",
        "\n",
        "In order to learn from experience, you need to accumulate the results of prior state-action transitions. In this case, you will use a 'q-table'. A q-table stores the relative quality of each state-action transition. The idea is simple: for a given state, the quality of each available action is represented by a numeric value. Higher values represent more favorable outcomes; lower values represent (relative) mistakes.\n",
        "\n",
        "For starters, to represent each space in the maze, you would need a data structure that is 4x4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_YhVHKp6331",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "\n",
        "'''\n",
        "create an array with one entry for\n",
        "each square on the maze.\n",
        "'''\n",
        "q = numpy.zeros((4,4))\n",
        "print(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxPkEJeZAvrz",
        "colab_type": "text"
      },
      "source": [
        "That array looks like the maze, but if you wanted to remember <code>(0,0)+N->bad</code>, where would you make that entry? The array includes an element (0,0), but that can only contain one value. If the maze is 4x4, then a 4x4 array could only store the value of a single action, for each state.\n",
        "\n",
        "What data structure would you use to store one value for each possible **state-action transition**?\n",
        "\n",
        "If a 4x4 array has one element per state, and each state has 4 available actions (N,S,E,W), how about trying 4x4x4? If you assign some numeric values to the actions (N=0, S=1, E=2, W=3), you could make entries to represent transitions like <code>(0,0)+N</code> (substituting 0 for N would be located at (0,0)+0, or: <code>[0][0][0]</code>): \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXoe1omUBvJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "'''\n",
        "a q-table that represents the quality of state-action transitions\n",
        "for a problem with 4x4 states and 4 actions\n",
        "'''\n",
        "\n",
        "env = Maze()\n",
        "state = env.reset()\n",
        "print('initial state', state)\n",
        "action = 0\n",
        "print('initial action', action)\n",
        "obs, reward, done = env.step(action)\n",
        "print('reward',reward)\n",
        "\n",
        "q = numpy.zeros((4,4,4))\n",
        "q[state][action] = reward    # remember the reward/penalty for (0,0)+N\n",
        "print('\\nHere is a q-table with one entry\\n')\n",
        "print(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxOQ6nCLCH7X",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing a Q-table\n",
        "That result may be correct, but it's hard to understand. Try using the maze environment to print the q-table in a more legible form. This format lists the states on the left-hand side, then lays out the actions across the columns. The idea is to clearly show that your one penalty is associated with state (0,0) and action=N:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN_3Fy1kCnJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Maze.print_q(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV87FX_gDWzd",
        "colab_type": "text"
      },
      "source": [
        "# Store the rewards / penalties from the random walks in the Q-table\n",
        "\n",
        "Now that you have a place to store rewards and penalties, take random walks through the maze until you find the exit, recording cumulative results in a Q-table:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDlXegzXDk2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "'''\n",
        "take random walks across the maze until you find the exit;\n",
        "accumulate all rewards / penalties in a q-table.\n",
        "'''\n",
        "env = Maze()\n",
        "state = env.reset()                      # start with the initial state\n",
        "q = np.zeros((4,4,4))\n",
        "total_steps = 0\n",
        "\n",
        "while True:\n",
        "    action = env.sample_n()              # a numeric action can serve as in index in the q-table\n",
        "    obs,reward,done = env.step(action)   # take a random step\n",
        "    q[state][action] += reward           # accumulate the reward or penalty\n",
        "    state = obs                          # next time, use the new state...\n",
        "    total_steps += 1                     # keep track of total number of steps\n",
        "    if done:\n",
        "        if reward>0:\n",
        "            break                        # success! you can stop now\n",
        "        else:\n",
        "            state = env.reset()          # failure! back to the initial state...\n",
        "\n",
        "Maze.print_q(q)\n",
        "print('...it only took', total_steps,'steps!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1i8irtQGfVC",
        "colab_type": "text"
      },
      "source": [
        "Take a look at the Q-table, and try to answer these questions:\n",
        "\n",
        "\n",
        "*   Why are the largest values negative, and on the first line?\n",
        "*   Can you find a positive value? From what state was it acheived?\n",
        "*   Why is there only one positive value?\n",
        "*   Why does the total number of steps exceed the sum of the values in the table?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1MeLGFPV5oz",
        "colab_type": "text"
      },
      "source": [
        "# Use the Q-table to avoid past mistakes\n",
        "\n",
        "What if you tried using the q-table to avoid repeating past mistakes? That algorithm is simple---if an action has resulted in a penalty on a prior attempt, avoid that action---but would that **always** work?\n",
        "\n",
        "Start by giving it a try:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaDNS-jgJ54m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "env = Maze()\n",
        "state = env.reset()                      \n",
        "q = np.zeros((4,4,4))    \n",
        "total_steps = 0\n",
        "\n",
        "while True:\n",
        "    \n",
        "    action = env.sample_n()       # pick a random action \n",
        "    while q[state][action]<0:     # if the action has resulted in a penalty...\n",
        "        action = env.sample_n()   # ...try something else.\n",
        "    \n",
        "    obs,reward,done = env.step(action)   \n",
        "    q[state][action] += reward           \n",
        "    total_steps += 1\n",
        "    state = obs                          \n",
        "    if done:\n",
        "        if reward>0:\n",
        "            break                        \n",
        "        else:\n",
        "            state = env.reset()          \n",
        "\n",
        "Maze.print_q(q)\n",
        "print('...it only took', total_steps,'steps!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9fWbs-qW4oE",
        "colab_type": "text"
      },
      "source": [
        "Hey! That's going in the right direction. However: your approach might fail in an environment where all actions lead to negative penalties, and your best path is to pick the **least bad action**. Also, you might get yourself caught in an infinite loop, if the maze has paths that lead back upon themselves. At this point, you have solve half the problem. You can avoid past penalties. Now you need to find your way *toward* positive rewards.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UxX810LXeRG",
        "colab_type": "text"
      },
      "source": [
        "# numpy to the rescue\n",
        "\n",
        "Whenever you bump into a slightly thorny issue, chances are that **numpy** has a function to help you out. In this case, you need to find **least bad action**. That will eliminate the possibility that your algorithm fails if all q-values are negative (but some less negative than others). Enter **numpy.argmax**, which returns the *position* of the greatest value in an array, take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3CeCmhEX7Ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def least_bad(v):\n",
        "    print('the position of the maximum value is:', np.argmax(v))\n",
        "\n",
        "least_bad([0,1,2,3,4,5])\n",
        "least_bad([-5,-4,-3,-2,-1])\n",
        "\n",
        "r = np.random.random((18))-.5\n",
        "print('\\n',r,'\\n')\n",
        "least_bad(r)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQiF7VYxYw9b",
        "colab_type": "text"
      },
      "source": [
        "# Learning on the fly: explore v. exploit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmCqI277Y0U7",
        "colab_type": "text"
      },
      "source": [
        "How about an approach that combines the random walk with avoiding past mistakes, so you can learn as you go? What if you use random moves to learn by *exploring* the maze, and then use **argmax** to *exploit* what you have learned? Here's a script that flips between exploration and exploitation, use a probability called **explore_rate**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQSWt9hWZlVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "env = Maze()\n",
        "state = env.reset()                      \n",
        "q = np.zeros((4,4,4))    \n",
        "total_steps = 0\n",
        "\n",
        "# experiment with different rates, in interval (0,1)\n",
        "explore_rate = 0.5\n",
        "\n",
        "while True:\n",
        "    \n",
        "    if np.random.random()<explore_rate:  # roll the dice...\n",
        "        action = env.sample_n()          # ...explore at random\n",
        "    else:                                # else...\n",
        "        action = np.argmax(q[state])     # ...pick best action, from experience\n",
        "        \n",
        "    obs,reward,done = env.step(action)   \n",
        "    q[state][action] += reward           \n",
        "    total_steps += 1\n",
        "    state = obs                          \n",
        "    if done:\n",
        "        if reward>0:\n",
        "            break                        \n",
        "        else:\n",
        "            state = env.reset()          \n",
        "\n",
        "Maze.print_q(q)\n",
        "print('...it only took', total_steps,'steps!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKgB2eqSgj4v",
        "colab_type": "text"
      },
      "source": [
        "# Optional: beware the exploding Q-table\n",
        "\n",
        "Your approach works, but some problems will require that you run your algorithm millions of times, or more. If you update your q-values by **accumulating** numbers, you may find that the values eventually *explode*, meaning: become too large to manage.\n",
        "\n",
        "To solve the maze, you will make a simplifying assumption in the next section. The remainder of this section describes a more complete, general approach to avoiding large q-values. **This section is optional, you won't need it to get to a complete understanding of Q-learning.**\n",
        "\n",
        "### OPTIONAL MATH STUFF: the learning rate $(\\alpha)$\n",
        "Many environments have complex reward structures, where the incidence and magnitude of rewards or penalities might be unknown, or might vary. In those cases, you would probably develop q-values by **combining** existing and new rewards using a constant $\\alpha$, something like: $q[s,a]=\\alpha \\times q[s,a]+(1-\\alpha) \\times reward$ where $0<\\alpha<1$. In that general case, $\\alpha$ is called the **learning rate**, because $\\alpha$ defines the proportion of old versus new information employed to update each value.\n",
        "\n",
        "For example: using the current algorithm, if you initially take action N a million times, your q-value for <code>(0,0)+N</code> would be -1,000,000. Using a learning rate $\\alpha=0.5$, your q-value for <code>(0,0)+N</code> would instead approach, but never equal, -1.0.\n",
        "\n",
        "### OPTIONAL COMPUTER SCIENCE STUFF: those math people are mixed up\n",
        "Did you read that **optional math stuff** paragraph? Somebody thinks that a computer can add numbers together forever and approach a limit. Hey! Somebody! Did no one ever mention that computers *don't really store continous floating-point numbers*? Was the whole ones-and-zero's thing lost on you? If you try your example, in Python, the sum will collapse to -1 because of the fundamental limitations of representing floating-point numbers:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hj_nQjOKOiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "this is a side-trip showing how math and CS don't always agree...\n",
        "it's OK to skip this example.\n",
        "'''\n",
        "alpha = 0.5\n",
        "sum = 0\n",
        "for n in range(1000000):\n",
        "    sum = alpha*sum + (1-alpha)*(-1)\n",
        "    if n%10==0:\n",
        "        print(n,sum)\n",
        "    if sum==-1.0:\n",
        "        print('see? told you so, it only took',n,'loops to equal',sum)\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZX_iBv4NL6ZI",
        "colab_type": "text"
      },
      "source": [
        "# Simplifying the Q-table\n",
        "\n",
        "The maze is a **deterministic** problem---the sets of states, actions, state-action transitions, and outcomes never changes. The one-and-only good move is to locate the exit, and all bad moves are equally bad (the game ends). Let's take advantage of that and simplify our q-values, to make the rest of our journey a little easier to understand. You learned a lot by accumulating q-values, because you could observe the **frequency** of each state-action pair. Going forward, you can put those values aside, and simply remember the most recent reward or penalty (you can get away with that because the rewards and penalties are equal and unchanging).\n",
        "\n",
        "You aren't dong that to throw away information --- your goal is to find a path toward a **future reward**, and this simplifying assumption will make that path easier to see."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPy4JntuhaUh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "env = Maze()\n",
        "state = env.reset()                      \n",
        "q = np.zeros((4,4,4))    \n",
        "total_steps = 0\n",
        "\n",
        "explore_rate = 0.5\n",
        "\n",
        "while True:\n",
        "    \n",
        "    if np.random.random()<explore_rate:\n",
        "        action = env.sample_n()       \n",
        "    else:                               \n",
        "        action = np.argmax(q[state])\n",
        "        \n",
        "    obs,reward,done = env.step(action)\n",
        "    q[state][action] = reward  # don't accumulate rewards / just store current value\n",
        "    total_steps += 1\n",
        "    state = obs\n",
        "    if done:\n",
        "        if reward>0:\n",
        "            break\n",
        "        else:\n",
        "            state = env.reset()\n",
        "\n",
        "Maze.print_q(q)\n",
        "print('...it only took', total_steps,'steps!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6-KnHOnOUON",
        "colab_type": "text"
      },
      "source": [
        "The simplified table is now a map of **good, bad, or indifferent** q-values. The remaining problem is: how could you discover the direction *toward* the one-and-only **good** value, starting at state <code>(0,0)</code>?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEn7QH4HcXJ7",
        "colab_type": "text"
      },
      "source": [
        "# What's special about state (2,3)?\n",
        "\n",
        "If you look back over the q-table values, the only positive reward is recorded from state (2,3). What's so special about state (2,3)?\n",
        "\n",
        "Take a closer look at state (2,3):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5uLrdNTct8M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Maze()\n",
        "env.mark = (2,3)\n",
        "print(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRRx5BY6Sh5y",
        "colab_type": "text"
      },
      "source": [
        "To reach the exit, you have to pass through state <code>(2,3)</code>, which is the same as *row 2, column 3*. From that position, moving South solves the maze. Go back to the q-table, and make note: the +1 reward is located at state <code>(2,3)+S</code>.\n",
        "\n",
        "When you see <code>(2,3)+S-> +1</code>, that implies the following: from state <code>(2,3)</code>, a positive reward is available... just choose action S. Or, let **argmax** choose for you, since +1 is greater than zero."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gT7k1kLRzgZ",
        "colab_type": "text"
      },
      "source": [
        "# What's special about state <code>(1,3)</code>?\n",
        "\n",
        "Now you are getting someplace. If state <code>(2,3)</code> offers a positive reward, and state <code>(2,3)</code> is available from state <code>(1,3)</code>, what does that imply about state <code>(1,3)</code>?\n",
        "\n",
        "Take a closer look at state <code>(1,3)</code>:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pIH4oBCR6OC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = Maze()\n",
        "env.mark = (1,3)\n",
        "print(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjv5FR9iUCFu",
        "colab_type": "text"
      },
      "source": [
        "### <code>(1,3)</code> on the first pass, prior to locating exit\n",
        "\n",
        "If you consider an action from state (1,3) on your first pass through the maze, here are your choices:\n",
        "* N = blocked square, penalty, game over!\n",
        "* S = open square, no score, play on\n",
        "* E = out of bounds, penalty, game over!\n",
        "* W = open square, no score, play on\n",
        "\n",
        "### <code>(1,3)</code> on the nth pass, after locating exit at least once\n",
        "If you have already encountered the exit at least once, and have therefore recorded a positive reward <code>(2,3)+S=+1</code>, then (1,3) looks like this:\n",
        "* N = blocked square, penalty, game over!\n",
        "* S = hey... that gets me to a positive reward @ (2,3)\n",
        "* E = out of bounds, penalty, game over!\n",
        "* W = open square, no score, play on\n",
        "\n",
        "Once you have found the exit, <code>(1,3)+S</code> offers a reward on the next step. If you can take an action that leads **toward** a reward, that's better than just wandering around (and way better than smacking into a block or stepping out of bounds).\n",
        "\n",
        "How could you adjust your the values in your q-table, to recognize that **future** reward?\n",
        "\n",
        "How about defining the q-value as the sum of (a+b), where:\n",
        "* a = whatever reward or penalty you received by taking an action\n",
        "* b = the maximum value available in your new state\n",
        "\n",
        "You could also write that as:\n",
        "* a = current reward, based on current experience\n",
        "* b = future reward, based on past experience\n",
        "\n",
        "In plain language: *heading toward a good thing is a good thing.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND-6eAH0Xnq_",
        "colab_type": "text"
      },
      "source": [
        "# Propagating rewards backwards\n",
        "\n",
        "Try altering your code to peek ahead one step. If there is a future reward available one step forward, add that future reward to the q-value. What happens if you run that until your q-table contains two positive rewards?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvyiWmgVXnO0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "env = Maze()\n",
        "state = env.reset()                      \n",
        "q = np.zeros((4,4,4))    \n",
        "total_steps = 0\n",
        "\n",
        "explore_rate = 0.5\n",
        "\n",
        "while True:\n",
        "    if np.random.random()<explore_rate: \n",
        "        action = env.sample_n()         \n",
        "    else:                               \n",
        "        action = np.argmax(q[state])\n",
        "        \n",
        "    obs,reward,done = env.step(action)\n",
        "\n",
        "    # when evaluating state+action, consider the max value of future state...\n",
        "    # ....so q value is (a+b), where a=current reward and b=future reward\n",
        "    q[state][action] = reward+np.max(q[obs])  # (a+b)\n",
        "    total_steps += 1\n",
        "    state = obs                          \n",
        "    if done:\n",
        "        if np.sum(q>0)==2: # how does this line manage to count the rewards?\n",
        "            break                        \n",
        "        else:\n",
        "            state = env.reset()          \n",
        "\n",
        "Maze.print_q(q)\n",
        "print('...it only took', total_steps,'steps!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCmOGsZ6fkU8",
        "colab_type": "text"
      },
      "source": [
        "Hey look! Now <code>(1,3)+S=+1</code>.\n",
        "\n",
        "You are onto something.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCns4_9lTx8u",
        "colab_type": "text"
      },
      "source": [
        "# Propogate rewards back to the start\n",
        "\n",
        "You should probably repeat that process until the positive values work their way back to the start:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rcyxzxVf_qk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "env = Maze()\n",
        "state = env.reset()                      \n",
        "q = np.zeros((4,4,4))    \n",
        "total_steps = 0\n",
        "\n",
        "explore_rate = 0.5\n",
        "\n",
        "while True:\n",
        "    if np.random.random()<explore_rate: \n",
        "        action = env.sample_n()         \n",
        "    else:                               \n",
        "        action = np.argmax(q[state])\n",
        "        \n",
        "    obs,reward,done = env.step(action)\n",
        "    q[state][action] = reward+np.max(q[obs])\n",
        "    total_steps += 1\n",
        "    state = obs                          \n",
        "    if done:\n",
        "        if max(q[(0,0)])>0: # the reward has propagated backward\n",
        "            break                        \n",
        "        else:\n",
        "            state = env.reset()          \n",
        "\n",
        "Maze.print_q(q)\n",
        "print('...it only took', total_steps,'steps!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWjCYxk3TjsD",
        "colab_type": "text"
      },
      "source": [
        "# Plot the positive Q-values\n",
        "\n",
        "Notice anything familiar about the shape?\n",
        "\n",
        "It's the path through the maze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw2MDRKzM2Cq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Maze.plot(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQeBeTfopvM_",
        "colab_type": "text"
      },
      "source": [
        "# Can you choose between two equal rewards?\n",
        "\n",
        "How would you use the q-table to find the shortest path to the exit of the maze?\n",
        "\n",
        "Once the table is populated, would that just be a matter of relying on <code>argmax</code> to choose the highest-value action, for any given state?\n",
        "\n",
        "Before you answer: imagine that you were transported into the **center** of a maze. There is a green line drawn on the floor, that connects the entrance to the exit. You can't see the whole line because it turns a corner in either direction. Would you know which way to go?\n",
        "\n",
        "Try using the current q-table (warning: this may or may not work, details to follow)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46egZdJffkJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "use the existing q-table to walk through the maze,\n",
        "by always selecting the highest-value action...\n",
        "\n",
        "NOTE: this code won't run unless the prior code that\n",
        "      populates the q-table has run during the current \n",
        "      session.\n",
        "'''\n",
        "\n",
        "env = Maze()\n",
        "state = env.reset()\n",
        "done = False\n",
        "circuit_breaker = 0   # don't run forever!\n",
        "while not done:\n",
        "    # use the q-table to select the highest value action for each state\n",
        "    obs,reward,done = env.step(np.argmax(q[state])) \n",
        "    state = obs\n",
        "    print(env)\n",
        "    circuit_breaker += 1\n",
        "    if circuit_breaker > 999:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhVL5nEr1wH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q[1][2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiB1YCSmrRuA",
        "colab_type": "text"
      },
      "source": [
        "If that worked, it was luck; there's a small chance of (complete) failure. If it failed, it probably ran for a very long time. Can you explain why?\n",
        "\n",
        "Hint: what would happen if you arbitrarily arranged the actions N,S,E,W in a different order, or if the maze had a loop in it? Is there a potential loop in the maze? If you dart back and forth between two spaces... is that a loop?\n",
        "\n",
        "Run this cell to see the shape of a failed q-table, and ask yourself: if two rewards are avaialable from <code>(0,0)</code>, which action does your algorithm choose? -- and then, which action?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vohRDxB5ugL-",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title An example of an (unlikely) Q-table failure\n",
        "import numpy as np\n",
        "\n",
        "env = Maze()\n",
        "state = env.reset()                      \n",
        "q = np.zeros((4,4,4))    \n",
        "total_steps = 0\n",
        "\n",
        "explore_rate = 0.5\n",
        "\n",
        "while True:\n",
        "    if np.random.random()<explore_rate: \n",
        "        action = env.sample_n()         \n",
        "    else:                               \n",
        "        action = np.argmax(q[state])\n",
        "        \n",
        "    obs,reward,done = env.step(action)\n",
        "    q[state][action] = reward+np.max(q[obs])\n",
        "    total_steps += 1\n",
        "    state = obs                          \n",
        "    if done:\n",
        "        if max(q[(1,0)])>0: # the unlikely but possible reward\n",
        "            break                        \n",
        "        else:\n",
        "            state = env.reset()          \n",
        "\n",
        "Maze.plot(q)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw30T6T-zddq",
        "colab_type": "text"
      },
      "source": [
        "In the example, above, an infinite loop can occur if the random search happens to record a reward at <code>(1,0)</code>. In that case, <code>argmax</code> select the **first** action with a reward of 1, which is **S**, then the **only** action with a reward of 1, which is **N**, then... **S,N,S,N...** forever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOo0_2Qbrgaf",
        "colab_type": "text"
      },
      "source": [
        "# Degenerate case: a maze with no obstacles\n",
        "\n",
        "To get a better look at the problem, let's use a maze with no obstacles. That's a *degenerate case* of a maze. Removing the obstacles takes away the distinction of being a maze. Now your environment is indistinguishable from the next broader set of possibilities... it's just an empty room:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CCwb-0dr8xV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "when is a maze not a maze? when it's just a room.\n",
        "'''\n",
        "env = Maze()\n",
        "env.remove_blocks()\n",
        "env.reset()\n",
        "print(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o770ML0ezoZv",
        "colab_type": "text"
      },
      "source": [
        "Go ahead and use your algorithm to traverse the empty room. Run this several times, to see the different shapes of the resulting solutions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxJTCcIYV0kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "'''\n",
        "traverse an empty room by removing the blocks from the maze\n",
        "'''\n",
        "\n",
        "env = Maze()\n",
        "env.remove_blocks()    # look ma, no blocks!\n",
        "\n",
        "q = np.zeros((4,4,4))\n",
        "total_steps = 0\n",
        "explore_rate = 0.5\n",
        "\n",
        "state = env.reset() \n",
        "while True:\n",
        "    if np.random.random()<explore_rate: \n",
        "        action = env.sample_n()         \n",
        "    else:                               \n",
        "        action = np.argmax(q[state])\n",
        " \n",
        "    obs,reward,done = env.step(action)\n",
        "    q[state][action] = reward+np.max(q[obs])\n",
        "    total_steps += 1\n",
        "    state = obs                          \n",
        "    if done:\n",
        "        if max(q[(0,0)])>0:\n",
        "            break                        \n",
        "        else:\n",
        "            state = env.reset()          \n",
        "\n",
        "Maze.plot(q)\n",
        "print('...it only took', total_steps,'steps!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIDPZ9ta0yWR",
        "colab_type": "text"
      },
      "source": [
        "Now try to use your q-table to walk across an empty room. You are very likely to fail. If you happen to succeed... rerun the q-table and try it again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CQDcDgl0mck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "follow the q-values... very unlikely to succeed!\n",
        "'''\n",
        "\n",
        "env = Maze()\n",
        "env.remove_blocks()\n",
        "state = env.reset()\n",
        "done = False\n",
        "circuit_breaker = 0   # don't run forever!\n",
        "while not done:\n",
        "    # use the q-table to select the highest value action for each state\n",
        "    obs,reward,done = env.step(np.argmax(q[state]))  \n",
        "    state = obs\n",
        "    print(env)\n",
        "    circuit_breaker += 1\n",
        "    if circuit_breaker > 999:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4cBZE4i0gfc",
        "colab_type": "text"
      },
      "source": [
        "See? You know there's a reward out there somewhere, but you don't know in which **direction** to go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5c-L9Q613Cc",
        "colab_type": "text"
      },
      "source": [
        "# Finding the direction toward a future reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Dv5btxI2T86",
        "colab_type": "text"
      },
      "source": [
        "When you noticed a future reward of +1, you propagated that backward by copying it to each immediately preceding state. By doing that repeatedly, you were able to represent a future reward at every intervening state, including, ultimately, the start. If the ultimate reward for solving a problem is +1, should the first step *toward* that reward also be +1? Or should you use a positive value than can represent both the availability of a future reward, and the direction toward the goal?\n",
        "\n",
        "Consider these two situations:\n",
        "\n",
        "* you are on a sidewalk; someone says *there is ice cream at the end of the sidewalk*; which way do you go?\n",
        "\n",
        "* you are on a staircase; someone says *there is ice cream at the top of the stairs*; which way do you go?\n",
        "\n",
        "Given those two choices: take the stairs.\n",
        "\n",
        "### Building a numerical staircase\n",
        "\n",
        "Take a look at your update to your q-table:\n",
        "\n",
        "* <code>q[state][action] = reward + np.max(q[obs])</code>\n",
        "\n",
        "Each q value has two components: <code>reward</code>, which is the immediate result of an action from the **current** state, and <code>max(q[obs])</code>, which is the highest value available from the **next** state. When you notice that the **next** state offers a maximum reward of +1, your code copies the +1 into the current q-value. You are building a sidewalk.\n",
        "\n",
        "To build a staircase, you need to shrink the value to something less than +1 (but still more than zero). That's like saying *a reward that is one step away is a good thing, but not as good as an immediate reward.*\n",
        "\n",
        "One simple way to shrink the reward is to multiply by a positive number that is less than 1, for example, $\\frac 3 4$:\n",
        "\n",
        "* <code>q[state][action] = reward + (3/4)*np.max(q[obs])</code>\n",
        "\n",
        "In that example, your rewards work be: 1.00, 0.75, 0.56, 0.42...\n",
        "\n",
        "Try it by running this code several times:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ow5JDFRYOv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "env = Maze()\n",
        "env.remove_blocks()\n",
        "\n",
        "q = np.zeros((4,4,4))\n",
        "total_steps = 0\n",
        "explore_rate = 0.5\n",
        "\n",
        "env.reset()\n",
        "while True:\n",
        "    if np.random.random()<explore_rate: \n",
        "        action = env.sample_n()         \n",
        "    else:                               \n",
        "        action = np.argmax(q[state])\n",
        "        \n",
        "    obs,reward,done = env.step(action)\n",
        "    q[state][action] = reward+(3/4)*np.max(q[obs]) # notice the 3/4?\n",
        "    total_steps += 1\n",
        "    state = obs                          \n",
        "    if done:\n",
        "        if max(q[(0,0)])>0:\n",
        "            break                        \n",
        "        else:\n",
        "            state = env.reset()          \n",
        "Maze.plot(q)\n",
        "print('...it only took', total_steps,'steps!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PYeW6fX7FXK",
        "colab_type": "text"
      },
      "source": [
        "See all those staircases? Each one leads to the exit.\n",
        "\n",
        "Now try to use your q-table to select the best action:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bNOSizJ7O6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "follow the q-values... up the stairs, to the exit.\n",
        "'''\n",
        "\n",
        "env = Maze()\n",
        "env.remove_blocks()\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # use the q-table to select the highest value action for each state\n",
        "    obs,reward,done = env.step(np.argmax(q[state]))  \n",
        "    state = obs\n",
        "    print(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfSHKp4I76oA",
        "colab_type": "text"
      },
      "source": [
        "# Putting it all together: up and out of the maze\n",
        "\n",
        "Now it's easy. You just need to stop removing the blocks from the maze.\n",
        "\n",
        "Take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnv09wM38Ho7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "env = Maze()\n",
        "\n",
        "q = np.zeros((4,4,4))\n",
        "total_steps = 0\n",
        "explore_rate = 0.5\n",
        "\n",
        "env.reset()\n",
        "while True:\n",
        "    if np.random.random()<explore_rate: \n",
        "        action = env.sample_n()         \n",
        "    else:                               \n",
        "        action = np.argmax(q[state])\n",
        "        \n",
        "    obs,reward,done = env.step(action)\n",
        "    q[state][action] = reward+(3/4)*np.max(q[obs])\n",
        "    total_steps += 1\n",
        "    state = obs                          \n",
        "    if done:\n",
        "        if max(q[(0,0)])>0:\n",
        "            break                        \n",
        "        else:\n",
        "            state = env.reset()          \n",
        "Maze.plot(q)\n",
        "print('...it only took', total_steps,'steps!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKSY3cUW8PW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "follow the q-values... up the stairs, to the exit.\n",
        "'''\n",
        "\n",
        "env = Maze()\n",
        "state = env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    # use the q-table to select the highest value action for each state\n",
        "    obs,reward,done = env.step(np.argmax(q[state]))  \n",
        "    state = obs\n",
        "    print(env)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}